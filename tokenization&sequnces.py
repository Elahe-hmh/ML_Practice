# -*- coding: utf-8 -*-
"""Tokenization&sequnces.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qksd7DlIq0ezQFTf_H9do5nyvSAhi1jI
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer

sentences=[
           'I love my dog',
           'I love my cat',
           'you love my dog!'
]
type(sentences)

tokenizer=Tokenizer(num_words=100)
tokenizer.fit_on_texts(sentences)
word_index=tokenizer.word_index

print(word_index)

"""creating sequences of numbers from sentences!

this time **sentences** are turned **to** sequences of **numbers**
"""

sentences2=[
           'I love my dog',
           'I love my cat',
           'you love my dog!',
           'Do you think my dog is amazing?'
]

tokenizer=Tokenizer(num_words=100)
tokenizer.fit_on_texts(sentences2)
word_index2=tokenizer.word_index

sequences=tokenizer.texts_to_sequences(sentences2)

print(word_index2)
print(sequences)

"""now all things are ready to classify, **but** there are words in text that neuralnetwork never has seen before!"""

test_data=[
           'I really love my dog',
           'my dog loves my manatee'
]

test_seq=tokenizer.texts_to_sequences(test_data)
test_seq
#because the word really is not in word index

#to correct the size if sentences, we use oov.token="<00V>" in tokenizer argumant
# it handles with sentences with different lenghth

sentences2=[
           'I love my dog',
           'I love my cat',
           'you love my dog!',
           'Do you think my dog is amazing?'
]

tokenizer=Tokenizer(num_words=100,oov_token="<00V>")
tokenizer.fit_on_texts(sentences2)
word_index2=tokenizer.word_index

sequences2=tokenizer.texts_to_sequences(sentences2)
print(word_index2)
print(sequences2)

test_seq=tokenizer.texts_to_sequences(test_data)
test_seq
#for every new word it writes 1 !!!

